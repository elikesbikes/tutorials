services:
  ollama:
    image: ollama/ollama:latest
    container_name: ${OLLAMA_NAME} # <-- PARAMETERIZED
    hostname: ${OLLAMA_NAME} # <-- PARAMETERIZED
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT}:11434" # <-- PARAMETERIZED host port
    volumes:
      - ${OLLAMA_DATA_PATH}:/root/.ollama # <-- PARAMETERIZED path
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - frontend

  openwebui:
    container_name: ${WEBUI_NAME} # <-- PARAMETERIZED
    hostname: ${WEBUI_NAME} # <-- PARAMETERIZED
    image: ghcr.io/open-webui/open-webui:latest
    restart: unless-stopped
    ports:
      - "${WEBUI_PORT}:8080" # <-- PARAMETERIZED host port
    volumes:
      - ${WEBUI_DATA_PATH}:/app/backend/data # <-- PARAMETERIZED path
      - ./openwebui_models:/app/backend/models
    environment:
      - OLLAMA_API_URL=http://${OLLAMA_NAME}:11434 # Uses parameterized name
      - COMMANDLINE_ARGS=--listen --port 8080
      - TZ=UTC
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - frontend
    depends_on:
      - ollama

#  ai-python-agent-prod-2:
#    image: python:3.12-slim
#    container_name: ${AGENT_NAME} # <-- PARAMETERIZED
#    hostname: ${AGENT_NAME} # <-- PARAMETERIZED
#    restart: unless-stopped
#    command: >
#        /bin/sh -c "pip install --no-cache-dir requests && python3 /app/agent.py > /app/output/llm_analysis_output.txt 2>&1"
#    #  command: >
#  #    /bin/sh -c "pip install --no-cache-dir requests && python3 /app/agent.py > /app/output/llm_analysis_output.txt 2>&1"
#    volumes:
#      - ./agent_output:/app/output
#      - ./agent:/app
#      - ./agent_logs:/app/logs
#
#    working_dir: /app
#    networks:
#      - frontend
#    depends_on:
#      - ollama
#    environment:
#      # --- Credentials and URLs (All read from .env) ---
#      - HOME_ASSISTANT_URL=${HOME_ASSISTANT_URL}
#      - HA_ACCESS_TOKEN=${HA_ACCESS_TOKEN}
#      - OLLAMA_API_URL=${OLLAMA_API_URL}
#      # --- CRITICAL FIX: Add the HOST variable the script reads ---
#      - OLLAMA_HOST=${OLLAMA_HOST}

networks:
  frontend:
    external: true
