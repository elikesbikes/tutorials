services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"        # Ollama API endpoint
    volumes:
      - ./ollama:/root/.ollama  # Persist model data
    environment:
      - OLLAMA_KEEP_ALIVE=24h   # Keep models loaded longer
    networks:
      - frontend                # Attach to your existing network
    command: >
      sh -c "
        # Pull Llama model on container start
        ollama pull llama3 &&
        # Start Ollama API server
        ollama api
      "
    # Uncomment these lines if you want GPU acceleration
    # runtime: nvidia
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=all
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]

networks:
  frontend:
    external: true
